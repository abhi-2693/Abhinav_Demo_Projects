{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95dcbd8",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "### Part 1: Understanding Neural Network Forward Propagation and Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4daa65",
   "metadata": {},
   "source": [
    "#### Package installation and Import statments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08dd26bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c970b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265eda68",
   "metadata": {},
   "source": [
    "##### 1. Implement forward propagation \n",
    "- Input and Xavier Initialization of Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a93b4151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Weights (Xavier Init):\n",
      "W_hidden [2 inputs -> 2 hidden] : [[w1, w2], [w3, w4]] = [[0.24835707650561634, -0.06913215058559233], [0.32384426905034625, 0.7615149282040127]]\n",
      "W_output [2 hidden -> 1 output] : [w5, w6] = [-0.15610224981555731, -0.15609130463278703]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42) # so that we get same random numbers\n",
    "\n",
    "# Inputs\n",
    "i1, i2 = 2, 4\n",
    "\n",
    "# Xavier initialization helper function\n",
    "def xavier_init(n_in, n_out):\n",
    "    sd = 2/(n_in+n_out)\n",
    "    return np.random.normal(0, sd, size=(n_out, n_in)) # to keep the Intial weight within[-1,1]\n",
    "\n",
    "# Initialize weights (2 inputs -> 2 hidden, 2 hidden -> 1 output)\n",
    "W_hidden_base = xavier_init(2, 2)   # shape (2,2) : {w1, w2, w3, w4}\n",
    "W_output_base = xavier_init(2, 1)   # shape (1,2) : {w5, w6}\n",
    "\n",
    "print(\"Initial Weights (Xavier Init):\")\n",
    "print(\"W_hidden [2 inputs -> 2 hidden] : [[w1, w2], [w3, w4]] =\", [W_hidden_base[0].tolist(), W_hidden_base[1].tolist()])\n",
    "print(\"W_output [2 hidden -> 1 output] : [w5, w6] =\", W_output_base[0].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69b92c3f",
   "metadata": {},
   "source": [
    "- Forward Propagation (Linear Activation Function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52c7f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for forward propagation using linear activation function in both Input -> HL -> Output Layer\n",
    "def forward_linear(X, W_hidden, W_output):\n",
    "    h = np.dot(W_hidden, X) # linear activation for 2D array for our case : [H1, H2] Hidden layer\n",
    "    out = np.dot(W_output, h) # output layer taking [H1, H2] as input\n",
    "    return h, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "174f16eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Forward Propagation (Linear Activation Function in Hidden layers and Output Layer):\n",
      "h_linear = [0.22018555 3.69374825]\n",
      "out_linear = [-0.61093344]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([i1, i2]) # intializing Input array\n",
    "W_hidden_linear = W_hidden_base.copy() # copying the hidden weights for the Linear activation function\n",
    "W_output_linear = W_output_base.copy() # copying the output weights\n",
    "\n",
    "h_linear, out_linear = forward_linear(X, W_hidden_linear, W_output_linear)\n",
    "\n",
    "print(\"\\nForward Propagation (Linear Activation Function in Hidden layers and Output Layer):\")\n",
    "print(\"h_linear =\", h_linear)\n",
    "print(\"out_linear =\", out_linear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d372690",
   "metadata": {},
   "source": [
    "#### 2 & 3. Derivation for Backpropagation and Weight Update\n",
    "- activation function for hidden layer : Linear\n",
    "- activation function for output layer : Linear\n",
    "- Loss Function : MSE\n",
    "- Learning rate : 0.05\n",
    "- True O/p : 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "525cc78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = 1.0 # actual output (Given)\n",
    "lr = 0.05 # learning rate (Given)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd31aaf",
   "metadata": {},
   "source": [
    "##### Backpropagation Derivation steps\n",
    "- Loss function MSE (L) = (1/2)*(sum((prediction - actual)^2)) # to hidden layer\n",
    "- dL = (prediction - actual) { first derivative of L w.r.t W_i }\n",
    "- W_i_update = W_i - lr * dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25c15266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "W_hidden [2 inputs -> 2 hidden] : [[w1, w2], [w3, w4]] = [[0.24835707650561634, -0.06913215058559233], [0.32384426905034625, 0.7615149282040127]]\n",
      "W_output [2 hidden -> 1 output] : [w5, w6] = [-0.15610224981555731, -0.15609130463278703]\n",
      "h_linear = [0.22018555 3.69374825]\n",
      "out_linear = [-0.61093344]\n",
      "\n",
      "error in final output = [-1.61093344]\n",
      "error in Hidden Layers Wieghts = [-0.35470427 -5.95038259]\n",
      "error in Input Layers Weights = [[-0.08809331485204337, 0.4113627451541031], [-0.11486894417456574, -4.531305169712425]]\n",
      "\n",
      "Updated W_hidden [2 inputs -> 2 hidden] : [[w1, w2], [w3, w4]] = [[0.2527617422482185, -0.08970028784329748], [0.3295877162590745, 0.9880801866896339]]\n",
      "Updated W_output [2 hidden -> 1 output] : [w5, w6] = [-0.1383670364502858, 0.14142782479513089]\n",
      "\n",
      "Epoch 1 Completed\n",
      "\n",
      "Forward Propagation (Linear hidden layer):\n",
      "Updated h_linear = [0.14672233 4.61149618]\n",
      "New out_linear = [0.63189234]\n"
     ]
    }
   ],
   "source": [
    "############# Initial Numbers in the same print statement #############\n",
    "print(\"\\nW_hidden [2 inputs -> 2 hidden] : [[w1, w2], [w3, w4]] =\", [W_hidden_linear[0].tolist(), W_hidden_linear[1].tolist()])\n",
    "print(\"W_output [2 hidden -> 1 output] : [w5, w6] =\", W_output_linear[0].tolist())\n",
    "print(\"h_linear =\", h_linear)\n",
    "print(\"out_linear =\", out_linear)\n",
    "############ end #############\n",
    "\n",
    "output_linear = [] # to save final output each EPOCH\n",
    "\n",
    "# Backpropagation loop - Created to see the EPOC evolution but printing only 1 epoch for the question\n",
    "for i in range(10):\n",
    "    output_linear.append((i,out_linear[0]))\n",
    "    dL = (out_linear - y_true)     # Calculating the error in the final output\n",
    "    dL_hl = dL * h_linear # Multiplying the error by the intermediate variable h_linear to calculate the error in the hidden layer weights\n",
    "    dL_in = dL_hl * W_hidden_linear # Multiplying the error in the hidden layer weights by the weights of the input layer to calculate the error in the input layer weights\n",
    "    \n",
    "    if i==0:\n",
    "        print(\"\\nerror in final output =\", dL)\n",
    "        print(\"error in Hidden Layers Wieghts =\", dL_hl)\n",
    "        print(\"error in Input Layers Weights =\", [dL_in[0].tolist(), dL_in[1].tolist()])\n",
    "\n",
    "    # Updating the weights in backpropagation using the given lr and calculated error at for different layers\n",
    "    W_hidden_linear = W_hidden_linear - lr * dL_in # [[w1, w2], [w3, w4]] \n",
    "    W_output_linear = W_output_linear - lr * dL_hl # [w5, w6]\n",
    "    \n",
    "    if i==0:\n",
    "        print(\"\\nUpdated W_hidden [2 inputs -> 2 hidden] : [[w1, w2], [w3, w4]] =\", [W_hidden_linear[0].tolist(), W_hidden_linear[1].tolist()])\n",
    "        print(\"Updated W_output [2 hidden -> 1 output] : [w5, w6] =\", W_output_linear[0].tolist())\n",
    "        print(\"\\nEpoch\", i+1, \"Completed\")\n",
    "    \n",
    "    # Using the Updated Weight in Forward propagation calculation to check the new O/p\n",
    "    h_linear, out_linear = forward_linear(X, W_hidden_linear, W_output_linear)\n",
    "\n",
    "    if i==0:\n",
    "        print(\"\\nForward Propagation (Linear hidden layer):\")\n",
    "        print(\"Updated h_linear =\", h_linear)\n",
    "        print(\"New out_linear =\", out_linear)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c8a53d",
   "metadata": {},
   "source": [
    "#### 4 & 5. Derivation for Backpropagation and Weight Update\n",
    "- activation function for hidden layer : ReLU\n",
    "- activation function for output layer : Linear\n",
    "- Loss Function : MSE\n",
    "- Learning rate : 0.05\n",
    "- True O/p : 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd51cfa9",
   "metadata": {},
   "source": [
    "Backpropagation Derivation steps remains the same as the loss function is same (MSE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfb823fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for forward propagation using ReLu activation function in Input -> HL and Linear as HL -> Output Layer\n",
    "def forward_relu(X, W_hidden, W_output):\n",
    "    h_pre = np.dot(W_hidden, X)\n",
    "    h = np.maximum(0, h_pre) # Appling the relu function only to the Hidden layer which takes the positive values as it is rest are set to 0\n",
    "    out = np.dot(W_output, h) # Output wights are still kept Linear\n",
    "    return h, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21bdd154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W_hidden [2 inputs -> 2 hidden] : [[w1, w2], [w3, w4]] = [[0.24835707650561634, -0.06913215058559233], [0.32384426905034625, 0.7615149282040127]]\n",
      "W_output [2 hidden -> 1 output] : [w5, w6] = [-0.15610224981555731, -0.15609130463278703]\n",
      "Hidden layer activations (ReLU): [0.22018555 3.69374825]\n",
      "Output: [-0.61093344]\n",
      "\n",
      "Error at output layer = [-1.61093344]\n",
      "Gradient at hidden layer = [-0.35470427 -5.95038259]\n",
      "Gradient at input layer = [[0.0, 0.4113627451541031], [0.0, 0.0]]\n",
      "\n",
      "Updated Weights:\n",
      "W_hidden [2 inputs -> 2 hidden] : [[w1, w2], [w3, w4]] = [[0.24835707650561634, -0.08970028784329748], [0.32384426905034625, 0.7615149282040127]]\n",
      "W_output [2 hidden -> 1 output] : [w5, w6] = [-0.1383670364502858, 0.14142782479513089]\n",
      "\n",
      "Epoch 1 Completed\n",
      "\n",
      "Forward Pass with Updated Weights:\n",
      "Updated hidden layer activations (ReLU): [0.137913   3.69374825]\n",
      "New network output: [0.50331617]\n"
     ]
    }
   ],
   "source": [
    "# Initialize input and weights\n",
    "X = np.array([i1, i2])  # Input \n",
    "W_hidden_relu = W_hidden_base.copy()  # Weights copy for ReLu setup input to hidden layer \n",
    "W_output_relu = W_output_base.copy()  # Weights Copy for ReLu setup hidden to output layer\n",
    "\n",
    "# Forward pass\n",
    "h_relu, out_relu = forward_relu(X, W_hidden_relu, W_output_relu)\n",
    "\n",
    "print(\"W_hidden [2 inputs -> 2 hidden] : [[w1, w2], [w3, w4]] =\", [W_hidden_relu[0].tolist(), W_hidden_relu[1].tolist()])\n",
    "print(\"W_output [2 hidden -> 1 output] : [w5, w6] =\", W_output_relu[0].tolist())\n",
    "print(\"Hidden layer activations (ReLU):\", h_relu)\n",
    "print(\"Output:\", out_relu)\n",
    "\n",
    "output_relu = [] # to save final output each EPOCH\n",
    "for i in range(10):\n",
    "    output_relu.append((i,out_relu[0]))\n",
    "    dL = (out_relu - y_true)      \n",
    "    # Backpropagate error to hidden layer\n",
    "    dL_hl = dL * h_relu  # Liner used for error of HL to output layer\n",
    "    dL_in = np.maximum(0, dL_hl * W_hidden_relu)  # ReLu used for error of input to HL layer\n",
    "    \n",
    "    if i == 0:\n",
    "        print(\"\\nError at output layer =\", dL)\n",
    "        print(\"Gradient at hidden layer =\", dL_hl)\n",
    "        print(\"Gradient at input layer =\", [dL_in[0].tolist(), dL_in[1].tolist()])\n",
    "\n",
    "    # Update weights \n",
    "    W_hidden_relu = W_hidden_relu - lr * dL_in  # Update hidden layer weights\n",
    "    W_output_relu = W_output_relu - lr * dL_hl  # Update output layer weights\n",
    "    \n",
    "    if i == 0:\n",
    "        print(\"\\nUpdated Weights:\")\n",
    "        print(\"W_hidden [2 inputs -> 2 hidden] : [[w1, w2], [w3, w4]] =\", [W_hidden_relu[0].tolist(), W_hidden_relu[1].tolist()])\n",
    "        print(\"W_output [2 hidden -> 1 output] : [w5, w6] =\", W_output_relu[0].tolist())\n",
    "\n",
    "    if i == 0: print(\"\\nEpoch\", i+1, \"Completed\")\n",
    "\n",
    "    h_relu, out_relu = forward_relu(X, W_hidden_relu, W_output_relu) # forward pass with updated weights\n",
    "    if i == 0:\n",
    "        print(\"\\nForward Pass with Updated Weights:\")\n",
    "        print(\"Updated hidden layer activations (ReLU):\", h_relu)\n",
    "        print(\"New network output:\", out_relu)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "144a706a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -0.6109334433069566),\n",
       " (1, 0.6318923392708231),\n",
       " (2, 1.1039604059348322),\n",
       " (3, 0.9530622196975309),\n",
       " (4, 1.0186922741009243),\n",
       " (5, 0.9921224293960885),\n",
       " (6, 1.0032459690499789),\n",
       " (7, 0.9986497371950417),\n",
       " (8, 1.0005594922719192),\n",
       " (9, 0.9997677924608643)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d18a25fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, -0.6109334433069566),\n",
       " (1, 0.5033161671433549),\n",
       " (2, 0.8470620510455015),\n",
       " (3, 0.9529630642661389),\n",
       " (4, 0.985540466404757),\n",
       " (5, 0.995555736690121),\n",
       " (6, 0.9986340862656211),\n",
       " (7, 0.9995802024068039),\n",
       " (8, 0.9998709807490244),\n",
       " (9, 0.9999603476946268)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_relu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060f97cd",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0465761d",
   "metadata": {},
   "source": [
    "We can see from EPOC wise evolution of the output that the output is getting closer to the desired output as the epochs increase.\n",
    "- Linear Function takes on either side of the true value and converges.\n",
    "- ReLu Function takes gradual incremental steps to reach the true value and converges.\n",
    "- Epoch 10 is enough are roughly same for bith methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa28a509",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (PyTorch)",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
