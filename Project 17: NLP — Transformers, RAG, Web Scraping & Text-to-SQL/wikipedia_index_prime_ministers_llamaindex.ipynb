{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "931cc38d",
      "metadata": {},
      "source": [
        "# Problems 2 - wikipedia_index_prime_ministers_llamaindex"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e292780",
      "metadata": {
        "id": "0e292780"
      },
      "source": [
        "\n",
        "Q2A â€” Index Wikipedia pages of all Prime Ministers of India with LlamaIndex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "da7014d9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install requests beautifulsoup4 llama-index openai tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "da389127",
      "metadata": {},
      "outputs": [],
      "source": [
        "# imports and constants\n",
        "import os\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "3e096120",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e096120",
        "outputId": "d0e67d59-241f-4b0d-a727-e36331ba052f"
      },
      "outputs": [],
      "source": [
        "# List of prime ministers to download (includes Gulzarilal Nanda as acting PM)\n",
        "PM_NAMES = [\n",
        "    \"Jawaharlal_Nehru\",\n",
        "    \"Lal_Bahadur_Shastri\",\n",
        "    \"Gulzarilal_Nanda\",\n",
        "    \"Indira_Gandhi\",\n",
        "    \"Morarji_Desai\",\n",
        "    \"Charan_Singh\",\n",
        "    \"Rajiv_Gandhi\",\n",
        "    \"V._P._Singh\",\n",
        "    \"Chandra_Shekhar\",\n",
        "    \"P._V._Narasimha_Rao\",\n",
        "    \"Atal_Bihari_Vajpayee\",\n",
        "    \"H._D._Deve_Gowda\",\n",
        "    \"Inder_Kumar_Gujral\",\n",
        "    \"Manmohan_Singh\",\n",
        "    \"Narendra_Modi\",\n",
        "]\n",
        "\n",
        "WIKI_BASE = \"https://en.wikipedia.org/wiki/\"\n",
        "OUT_DIR = Path(\"wikipedia_pm_pages\")\n",
        "OUT_DIR.mkdir(exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "17037844",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17037844",
        "outputId": "c56b45c6-9bc6-4a07-e09f-50bd1bd4a92e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://en.wikipedia.org/wiki/Jawaharlal_Nehru\n",
            "Downloading https://en.wikipedia.org/wiki/Lal_Bahadur_Shastri\n",
            "Downloading https://en.wikipedia.org/wiki/Gulzarilal_Nanda\n",
            "Downloading https://en.wikipedia.org/wiki/Indira_Gandhi\n",
            "Downloading https://en.wikipedia.org/wiki/Morarji_Desai\n",
            "Downloading https://en.wikipedia.org/wiki/Charan_Singh\n",
            "Downloading https://en.wikipedia.org/wiki/Rajiv_Gandhi\n",
            "Downloading https://en.wikipedia.org/wiki/V._P._Singh\n",
            "Downloading https://en.wikipedia.org/wiki/Chandra_Shekhar\n",
            "Downloading https://en.wikipedia.org/wiki/P._V._Narasimha_Rao\n",
            "Downloading https://en.wikipedia.org/wiki/Atal_Bihari_Vajpayee\n",
            "Downloading https://en.wikipedia.org/wiki/H._D._Deve_Gowda\n",
            "Downloading https://en.wikipedia.org/wiki/Inder_Kumar_Gujral\n",
            "Downloading https://en.wikipedia.org/wiki/Manmohan_Singh\n",
            "Downloading https://en.wikipedia.org/wiki/Narendra_Modi\n",
            "Downloaded pages saved to wikipedia_pm_pages\n"
          ]
        }
      ],
      "source": [
        "def download_and_extract_wiki(name):\n",
        "    url = WIKI_BASE + name\n",
        "    print(f\"Downloading {url}\")\n",
        "    # Add a User-Agent header to mimic a browser, which can help bypass 403 errors\n",
        "    headers = {\n",
        "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
        "    }\n",
        "    r = requests.get(url, timeout=30, headers=headers)\n",
        "    r.raise_for_status()\n",
        "    soup = BeautifulSoup(r.text, \"html.parser\")\n",
        "    # Remove tables of contents, navboxes, references, and scripts/styles\n",
        "    for el in soup([\"script\", \"style\", \"aside\", \"footer\", \"nav\", \"sup\", \"table\", \"style\"]):\n",
        "        el.decompose()\n",
        "    content = soup.find(\"div\", {\"class\": \"mw-parser-output\"})\n",
        "    if content is None:\n",
        "        # fallback: grab the whole text\n",
        "        text = soup.get_text(separator=\"\\n\")\n",
        "        return text\n",
        "    # Collect paragraph text under main content\n",
        "    paras = []\n",
        "    for p in content.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5']):\n",
        "        txt = p.get_text(strip=True)\n",
        "        if txt:\n",
        "            paras.append(txt)\n",
        "    return \"\\n\\n\".join(paras)\n",
        "\n",
        "downloaded = {}\n",
        "for name in PM_NAMES:\n",
        "    try:\n",
        "        text = download_and_extract_wiki(name)\n",
        "        fn = OUT_DIR / f\"{name}.txt\"\n",
        "        fn.write_text(text, encoding='utf-8')\n",
        "        downloaded[name] = str(fn)\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to download {name}: {e}\")\n",
        "print(\"Downloaded pages saved to\", OUT_DIR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "39bfdd7c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39bfdd7c",
        "outputId": "c015900f-7028-4094-fefc-8687fb6ad9fd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LLM is explicitly disabled. Using MockLLM.\n",
            "Total chunks (by token splitter with chunk_size=1024, overlap=20): 15\n",
            "Wrote q2a_index_summary.json\n"
          ]
        }
      ],
      "source": [
        "# Indexing with LlamaIndex and counting chunks\n",
        "# NOTE: llama-index API can vary by version. The snippet below follows modern llama-index patterns.\n",
        "# If API differences arise, adapt to your installed llama-index version.\n",
        "\n",
        "from llama_index.core import Document\n",
        "from llama_index.core.node_parser import TokenTextSplitter\n",
        "from llama_index.embeddings.openai import OpenAIEmbedding\n",
        "from llama_index.core import Settings\n",
        "import json\n",
        "\n",
        "# --- Configure embedding model (OpenAI text-embedding-3-large) ---\n",
        "embed_model = OpenAIEmbedding(model=\"text-embedding-3-large\")\n",
        "\n",
        "# --- Configure LLM (llama-3.1-8b-instant) ---\n",
        "# How to plug in llama-3.1-8b-instant depends on your LLM provider wrapper.\n",
        "# Example (pseudo-code) if using an OpenAI-compatible provider:\n",
        "# from llama_index.llms import OpenAI\n",
        "# llm = OpenAI(model_name=\"llama-3.1-8b-instant\", temperature=0)\n",
        "#\n",
        "# If you have a provider wrapper for 'llama' (for example, MosaicML or other),\n",
        "# replace the LLMPredictor below accordingly.\n",
        "\n",
        "# For safety, we set llm to None here; you should configure your LLM wrapper in your environment.\n",
        "llm = None\n",
        "\n",
        "# Configure global settings for LlamaIndex\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "Settings.chunk_size = 1024      # token chunk size (default in docs)\n",
        "Settings.chunk_overlap = 20     # token overlap (default in docs)\n",
        "\n",
        "# Read downloaded files into LlamaIndex Documents\n",
        "docs = []\n",
        "for name, path in downloaded.items():\n",
        "    text = Path(path).read_text(encoding='utf-8')\n",
        "    doc = Document(text_content=text) # Create Document with only the text content\n",
        "    doc.metadata = {\"source\": name} # Assign metadata separately\n",
        "    docs.append(doc)\n",
        "\n",
        "# Use TokenTextSplitter (token-based) for splitting into nodes/chunks\n",
        "token_splitter = TokenTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
        "all_nodes = []\n",
        "for doc in docs:\n",
        "    nodes = token_splitter.split_text(doc.get_content())\n",
        "    # split_text returns list of dicts or nodes depending on version; adapt if necessary\n",
        "    # We'll normalize to text chunks\n",
        "    if isinstance(nodes, list) and len(nodes) and isinstance(nodes[0], dict) and 'text' in nodes[0]:\n",
        "        chunk_texts = [n['text'] for n in nodes]\n",
        "    else:\n",
        "        # assume list of strings\n",
        "        chunk_texts = nodes\n",
        "    all_nodes.extend(chunk_texts)\n",
        "\n",
        "print(f\"Total chunks (by token splitter with chunk_size=1024, overlap=20): {len(all_nodes)}\")\n",
        "\n",
        "# --- Build index (optional) ---\n",
        "# If you have LLM configured, you can build a vector index:\n",
        "# index = GPTVectorStoreIndex.from_documents(docs, service_context=service_context)\n",
        "# index.storage_context.persist(persist_dir='q2a_index')\n",
        "#\n",
        "# Save a small summary JSON with counts:\n",
        "summary = {\n",
        "    \"num_documents\": len(docs),\n",
        "    \"num_chunks\": len(all_nodes),\n",
        "    \"chunk_size_tokens\": 1024,\n",
        "    \"chunk_overlap_tokens\": 20,\n",
        "    \"pm_list\": list(downloaded.keys())\n",
        "}\n",
        "Path('q2a_index_summary.json').write_text(json.dumps(summary, indent=2), encoding='utf-8')\n",
        "print('Wrote q2a_index_summary.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04f7fc5f",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
