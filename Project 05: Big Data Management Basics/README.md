# Project 05: Big Data Management Basics (PySpark)

## Situation / Objective
Modern analytics workflows often exceed single-machine processing constraints. The objective is to learn foundational big data processing concepts using PySpark, including data ingestion and transformation patterns.

## Task
- Load multiple datasets.
- Perform common transformations and exploratory checks.
- Apply Spark concepts such as RDD-style operations.

## Actions
- Ingested datasets into dataframes and validated shapes/nulls.
- Practiced transformation patterns typically used in big data pipelines.
- Worked through Spark-oriented operations and reasoning about distributed processing.

## Results / Summary
- Built familiarity with PySpark workflow foundations and the mental model behind distributed transformations.
- Established a reusable notebook for basic big data ingestion/cleaning steps.

## Repository contents
- `Big_data_management_basics.ipynb`
