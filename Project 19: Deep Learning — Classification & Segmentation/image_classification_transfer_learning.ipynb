{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM_kGTjyq_3L"
      },
      "source": [
        "# Image classification using transfer learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "HE2GuebuhhNt"
      },
      "outputs": [],
      "source": [
        "# Install required Python packages (quiet mode)\n",
        "!pip install -q torch torchvision scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "7oC_PCh7hkEV"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os, zipfile, shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnuN5E_Mhmmf",
        "outputId": "f6bc7209-a91d-4a35-8964-bd8b27be4f3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Select CPU or GPU device for computations\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zDtDV8Zhrog",
        "outputId": "5fe433d2-6db1-4fdd-d037-fe29615787b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['accordion', 'gramophone', 'windsor_chair', 'pyramid', 'pizza', 'sea_horse', 'crocodile', 'camera', 'emu', 'crocodile_head', 'cup', 'bass', 'dollar_bill', 'nautilus', 'hedgehog']\n"
          ]
        }
      ],
      "source": [
        "# Extract the ZIP dataset once and set DATASET_PATH\n",
        "with zipfile.ZipFile(\"/content/dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")\n",
        "\n",
        "DATASET_PATH = \"/content/dataset\"\n",
        "print(os.listdir(DATASET_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "wYRbxCoHh5tR"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training (images 0001-0040) and test (remaining) subsets based on filename\n",
        "\n",
        "def split_by_filename(dataset):\n",
        "    \"\"\"\n",
        "    Split the dataset into training and test subsets based on the filenames of the images.\n",
        "    The images with filenames ranging from 'image_0001.jpg' to 'image_0040.jpg' are considered\n",
        "    as training images, while the rest are considered as test images.\n",
        "\n",
        "    Args:\n",
        "        dataset (torchvision.datasets.ImageFolder): The dataset to be split.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two Subset objects. The first Subset contains the training images,\n",
        "        and the second Subset contains the test images.\n",
        "    \"\"\"\n",
        "    train_idx, test_idx = [], []\n",
        "    # Iterate over each image in the dataset\n",
        "    for idx, (path, _) in enumerate(dataset.samples):\n",
        "        # Extract the filename from the path\n",
        "        fname = os.path.basename(path)\n",
        "        # Extract the numeric part from the filename\n",
        "        # For example, 'image_0001.jpg' â†’ 1\n",
        "        num = int(os.path.splitext(fname)[0].replace('image_', ''))\n",
        "        # Check if the numeric part is between 1 and 40 (inclusive)\n",
        "        if 1 <= num <= 40:\n",
        "            # If it is, add the index to the training indices list\n",
        "            train_idx.append(idx)\n",
        "        else:\n",
        "            # If it is not, add the index to the test indices list\n",
        "            test_idx.append(idx)\n",
        "    # Return the training and test subsets of the dataset\n",
        "    return Subset(dataset, train_idx), Subset(dataset, test_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Nsl_3bnah8Wi"
      },
      "outputs": [],
      "source": [
        "# Resize all images to 224x224\n",
        "# Apply random horizontal flip to some images during training\n",
        "# Convert PIL Image to tensor\n",
        "# Normalize the tensor image using mean and standard deviation of ImageNet images\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# Resize all images to 224x224\n",
        "# Convert PIL Image to tensor\n",
        "# Normalize the tensor image using mean and standard deviation of ImageNet images\n",
        "test_tfms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# Load the dataset from the specified path\n",
        "# Apply the specified transforms to the images\n",
        "full_dataset = datasets.ImageFolder(DATASET_PATH, transform=train_tfms)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_set, test_set = split_by_filename(full_dataset)\n",
        "\n",
        "# Apply the test transforms to the test set\n",
        "test_set.dataset.transform = test_tfms\n",
        "\n",
        "# Create data loaders for the train and test sets\n",
        "# Load images in batches of size 32\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 15\n",
        "\n",
        "# Names of the classes in the dataset\n",
        "class_names = full_dataset.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "eC-nXoajiG7c"
      },
      "outputs": [],
      "source": [
        "# Utility functions: training loop and evaluation (accuracy & classification report)\n",
        "\n",
        "def train_model(model, epochs=10, lr=1e-4):\n",
        "    \"\"\"\n",
        "    Trains the given model on the training data for a specified number of epochs.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to be trained.\n",
        "        epochs (int, optional): The number of epochs to train the model for. Defaults to 10.\n",
        "        lr (float, optional): The learning rate for the optimizer. Defaults to 1e-4.\n",
        "    \"\"\"\n",
        "    model.to(device)  # Move the model to the device (GPU or CPU)\n",
        "    criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)  # Define the optimizer\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0  # Initialize the running loss\n",
        "        for x, y in train_loader:  # Iterate over the training data batch by batch\n",
        "            x, y = x.to(device), y.to(device)  # Move the data to the device\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            loss = criterion(model(x), y)  # Compute the loss\n",
        "            loss.backward()  # Compute the gradients\n",
        "            optimizer.step()  # Update the model parameters\n",
        "            running_loss += loss.item()  # Add the batch loss to the running loss\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")  # Print the epoch loss\n",
        "\n",
        "def evaluate_model(model):\n",
        "    \"\"\"\n",
        "    Evaluates the given model on the test data and prints the classification report.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to be evaluated.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    y_true, y_pred = [], []  # Initialize lists to store true labels and predicted labels\n",
        "    with torch.no_grad():  # Disable gradient computation to save memory\n",
        "        for x, y in test_loader:  # Iterate over the test data batch by batch\n",
        "            x = x.to(device)  # Move the data to the device\n",
        "            preds = torch.argmax(model(x), dim=1)  # Get the predicted labels\n",
        "            y_true.extend(y.numpy())  # Add the true labels to the list\n",
        "            y_pred.extend(preds.cpu().numpy())  # Add the predicted labels to the list\n",
        "\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))  # Print the classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZszECwcq_3a"
      },
      "source": [
        "# ===================== Q1 ====================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LW4aC8F7q_3a"
      },
      "source": [
        "### ResNet18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pTvfy5Tj3PB",
        "outputId": "8a02c491-3ed7-4f0f-9b05-b29f1ea0d472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 1.4212\n",
            "Epoch [2/10], Loss: 0.2031\n",
            "Epoch [3/10], Loss: 0.0659\n",
            "Epoch [4/10], Loss: 0.0318\n",
            "Epoch [5/10], Loss: 0.0210\n",
            "Epoch [6/10], Loss: 0.0152\n",
            "Epoch [7/10], Loss: 0.0124\n",
            "Epoch [8/10], Loss: 0.0109\n",
            "Epoch [9/10], Loss: 0.0097\n",
            "Epoch [10/10], Loss: 0.0073\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     accordion     1.0000    1.0000    1.0000        15\n",
            "          bass     1.0000    1.0000    1.0000        14\n",
            "        camera     1.0000    1.0000    1.0000        10\n",
            "     crocodile     0.9000    0.9000    0.9000        10\n",
            "crocodile_head     0.9091    0.9091    0.9091        11\n",
            "           cup     1.0000    1.0000    1.0000        17\n",
            "   dollar_bill     1.0000    1.0000    1.0000        12\n",
            "           emu     1.0000    1.0000    1.0000        13\n",
            "    gramophone     1.0000    1.0000    1.0000        11\n",
            "      hedgehog     0.9286    0.9286    0.9286        14\n",
            "      nautilus     0.9375    1.0000    0.9677        15\n",
            "         pizza     1.0000    1.0000    1.0000        13\n",
            "       pyramid     1.0000    1.0000    1.0000        17\n",
            "     sea_horse     1.0000    0.9412    0.9697        17\n",
            " windsor_chair     1.0000    1.0000    1.0000        16\n",
            "\n",
            "      accuracy                         0.9805       205\n",
            "     macro avg     0.9783    0.9786    0.9783       205\n",
            "  weighted avg     0.9808    0.9805    0.9805       205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 1. Pretrained models (No Finetuning)\n",
        "# 2. Here we are using the ResNet18 architecture from torchvision.models\n",
        "# 3. The ResNet18 architecture is a pre-trained deep neural network architecture\n",
        "#    that is commonly used for image classification tasks.\n",
        "# 4. The pretrained=True argument loads the model with the weights pre-trained\n",
        "#    on the ImageNet dataset.\n",
        "# 5. We are replacing the last fully connected layer (fc) with a new one that\n",
        "#    has the same number of input features as the original fc layer and the\n",
        "#    number of output features as the number of classes in our dataset (num_classes).\n",
        "# 6. The train_model function is used to train the model on the training data.\n",
        "# 7. The evaluate_model function is used to evaluate the model on the test data.\n",
        "\n",
        "resnet18 = models.resnet18(pretrained=True)\n",
        "resnet18.fc = nn.Linear(resnet18.fc.in_features, num_classes)\n",
        "train_model(resnet18)\n",
        "evaluate_model(resnet18)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpynvnHMq_3e"
      },
      "source": [
        "### DenseNet121"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGw2MiUJkMTa",
        "outputId": "d8ca9a2c-57b9-440c-cba9-799f1521720a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 1.8814\n",
            "Epoch [2/10], Loss: 0.5692\n",
            "Epoch [3/10], Loss: 0.2127\n",
            "Epoch [4/10], Loss: 0.1071\n",
            "Epoch [5/10], Loss: 0.0702\n",
            "Epoch [6/10], Loss: 0.0449\n",
            "Epoch [7/10], Loss: 0.0409\n",
            "Epoch [8/10], Loss: 0.0317\n",
            "Epoch [9/10], Loss: 0.0249\n",
            "Epoch [10/10], Loss: 0.0181\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     accordion     1.0000    1.0000    1.0000        15\n",
            "          bass     1.0000    1.0000    1.0000        14\n",
            "        camera     1.0000    1.0000    1.0000        10\n",
            "     crocodile     0.9091    1.0000    0.9524        10\n",
            "crocodile_head     0.9091    0.9091    0.9091        11\n",
            "           cup     1.0000    1.0000    1.0000        17\n",
            "   dollar_bill     1.0000    1.0000    1.0000        12\n",
            "           emu     1.0000    1.0000    1.0000        13\n",
            "    gramophone     1.0000    1.0000    1.0000        11\n",
            "      hedgehog     1.0000    1.0000    1.0000        14\n",
            "      nautilus     1.0000    1.0000    1.0000        15\n",
            "         pizza     1.0000    1.0000    1.0000        13\n",
            "       pyramid     1.0000    1.0000    1.0000        17\n",
            "     sea_horse     1.0000    0.9412    0.9697        17\n",
            " windsor_chair     1.0000    1.0000    1.0000        16\n",
            "\n",
            "      accuracy                         0.9902       205\n",
            "     macro avg     0.9879    0.9900    0.9887       205\n",
            "  weighted avg     0.9907    0.9902    0.9903       205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Here we are using a pre-trained DenseNet121 model and fine-tuning it for our task of classifying images\n",
        "\n",
        "# Load the pre-trained model\n",
        "densenet121 = models.densenet121(pretrained=True)\n",
        "\n",
        "# Change the last fully connected layer to have the number of classes we have\n",
        "densenet121.classifier = nn.Linear(densenet121.classifier.in_features, num_classes)\n",
        "\n",
        "# Train the model on our training data\n",
        "train_model(densenet121)\n",
        "\n",
        "# Evaluate the model on our test data\n",
        "evaluate_model(densenet121)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsVPuPeDq_3h"
      },
      "source": [
        "### VGG19 (Pretrained Only)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Re011n4wkQ3E",
        "outputId": "b37e3d7e-8b42-4953-cdef-dc17f5170832"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 2.4815\n",
            "Epoch [2/10], Loss: 1.6802\n",
            "Epoch [3/10], Loss: 1.1707\n",
            "Epoch [4/10], Loss: 0.8300\n",
            "Epoch [5/10], Loss: 0.6664\n",
            "Epoch [6/10], Loss: 0.5414\n",
            "Epoch [7/10], Loss: 0.4494\n",
            "Epoch [8/10], Loss: 0.4190\n",
            "Epoch [9/10], Loss: 0.3462\n",
            "Epoch [10/10], Loss: 0.3095\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     accordion     1.0000    1.0000    1.0000        15\n",
            "          bass     1.0000    1.0000    1.0000        14\n",
            "        camera     1.0000    1.0000    1.0000        10\n",
            "     crocodile     0.7000    0.7000    0.7000        10\n",
            "crocodile_head     0.7500    0.8182    0.7826        11\n",
            "           cup     0.9444    1.0000    0.9714        17\n",
            "   dollar_bill     1.0000    1.0000    1.0000        12\n",
            "           emu     1.0000    1.0000    1.0000        13\n",
            "    gramophone     1.0000    1.0000    1.0000        11\n",
            "      hedgehog     1.0000    0.9286    0.9630        14\n",
            "      nautilus     0.9333    0.9333    0.9333        15\n",
            "         pizza     1.0000    1.0000    1.0000        13\n",
            "       pyramid     1.0000    0.9412    0.9697        17\n",
            "     sea_horse     1.0000    0.9412    0.9697        17\n",
            " windsor_chair     0.9412    1.0000    0.9697        16\n",
            "\n",
            "      accuracy                         0.9561       205\n",
            "     macro avg     0.9513    0.9508    0.9506       205\n",
            "  weighted avg     0.9579    0.9561    0.9565       205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Load pre-trained VGG19 model\n",
        "vgg19 = models.vgg19(pretrained=True)\n",
        "\n",
        "# Freeze all the layers in the model\n",
        "for param in vgg19.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Replace the last layer of the classifier with a new linear layer\n",
        "# The new layer has num_classes output units\n",
        "vgg19.classifier[6] = nn.Linear(4096, num_classes)\n",
        "\n",
        "# Train the model on our new dataset\n",
        "train_model(vgg19)\n",
        "\n",
        "# Evaluate the model on the test dataset\n",
        "evaluate_model(vgg19)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
