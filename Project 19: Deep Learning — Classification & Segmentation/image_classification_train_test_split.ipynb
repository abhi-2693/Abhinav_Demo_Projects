{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxp6mX_QrCIq"
      },
      "source": [
        "# Image Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HE2GuebuhhNt"
      },
      "outputs": [],
      "source": [
        "# Install required Python packages (quiet mode)\n",
        "!pip install -q torch torchvision scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7oC_PCh7hkEV"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os, zipfile, shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import classification_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnuN5E_Mhmmf",
        "outputId": "2b0ab5c8-14f3-4666-cd3f-e935c4383475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "# Select CPU or GPU device for computations\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4zDtDV8Zhrog",
        "outputId": "180b739d-1eca-47ce-d78b-6186f64c9c52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['accordion', 'gramophone', 'windsor_chair', 'pyramid', 'pizza', 'sea_horse', 'crocodile', 'camera', 'emu', 'crocodile_head', 'cup', 'bass', 'dollar_bill', 'nautilus', 'hedgehog']\n"
          ]
        }
      ],
      "source": [
        "# Extract the ZIP dataset once and set DATASET_PATH\n",
        "with zipfile.ZipFile(\"/content/dataset.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content\")\n",
        "\n",
        "DATASET_PATH = \"/content/dataset\"\n",
        "print(os.listdir(DATASET_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wYRbxCoHh5tR"
      },
      "outputs": [],
      "source": [
        "# Split dataset into training (images 0001-0040) and test (remaining) subsets based on filename\n",
        "\n",
        "def split_by_filename(dataset):\n",
        "    \"\"\"\n",
        "    Split the dataset into training and test subsets based on the filenames of the images.\n",
        "    The images with filenames ranging from 'image_0001.jpg' to 'image_0040.jpg' are considered\n",
        "    as training images, while the rest are considered as test images.\n",
        "\n",
        "    Args:\n",
        "        dataset (torchvision.datasets.ImageFolder): The dataset to be split.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing two Subset objects. The first Subset contains the training images,\n",
        "        and the second Subset contains the test images.\n",
        "    \"\"\"\n",
        "    train_idx, test_idx = [], []\n",
        "    # Iterate over each image in the dataset\n",
        "    for idx, (path, _) in enumerate(dataset.samples):\n",
        "        # Extract the filename from the path\n",
        "        fname = os.path.basename(path)\n",
        "        # Extract the numeric part from the filename\n",
        "        # For example, 'image_0001.jpg' → 1\n",
        "        num = int(os.path.splitext(fname)[0].replace('image_', ''))\n",
        "        # Check if the numeric part is between 1 and 40 (inclusive)\n",
        "        if 1 <= num <= 40:\n",
        "            # If it is, add the index to the training indices list\n",
        "            train_idx.append(idx)\n",
        "        else:\n",
        "            # If it is not, add the index to the test indices list\n",
        "            test_idx.append(idx)\n",
        "    # Return the training and test subsets of the dataset\n",
        "    return Subset(dataset, train_idx), Subset(dataset, test_idx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Nsl_3bnah8Wi"
      },
      "outputs": [],
      "source": [
        "# Resize all images to 224x224\n",
        "# Apply random horizontal flip to some images during training\n",
        "# Convert PIL Image to tensor\n",
        "# Normalize the tensor image using mean and standard deviation of ImageNet images\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# Resize all images to 224x224\n",
        "# Convert PIL Image to tensor\n",
        "# Normalize the tensor image using mean and standard deviation of ImageNet images\n",
        "test_tfms = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# Load the dataset from the specified path\n",
        "# Apply the specified transforms to the images\n",
        "full_dataset = datasets.ImageFolder(DATASET_PATH, transform=train_tfms)\n",
        "\n",
        "# Split the dataset into train and test sets\n",
        "train_set, test_set = split_by_filename(full_dataset)\n",
        "\n",
        "# Apply the test transforms to the test set\n",
        "test_set.dataset.transform = test_tfms\n",
        "\n",
        "# Create data loaders for the train and test sets\n",
        "# Load images in batches of size 32\n",
        "train_loader = DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 15\n",
        "\n",
        "# Names of the classes in the dataset\n",
        "class_names = full_dataset.classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eC-nXoajiG7c"
      },
      "outputs": [],
      "source": [
        "# Utility functions: training loop and evaluation (accuracy & classification report)\n",
        "\n",
        "def train_model(model, epochs=10, lr=1e-4):\n",
        "    \"\"\"\n",
        "    Trains the given model on the training data for a specified number of epochs.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to be trained.\n",
        "        epochs (int, optional): The number of epochs to train the model for. Defaults to 10.\n",
        "        lr (float, optional): The learning rate for the optimizer. Defaults to 1e-4.\n",
        "    \"\"\"\n",
        "    model.to(device)  # Move the model to the device (GPU or CPU)\n",
        "    criterion = nn.CrossEntropyLoss()  # Define the loss function\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)  # Define the optimizer\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "        running_loss = 0  # Initialize the running loss\n",
        "        for x, y in train_loader:  # Iterate over the training data batch by batch\n",
        "            x, y = x.to(device), y.to(device)  # Move the data to the device\n",
        "            optimizer.zero_grad()  # Zero the gradients\n",
        "            loss = criterion(model(x), y)  # Compute the loss\n",
        "            loss.backward()  # Compute the gradients\n",
        "            optimizer.step()  # Update the model parameters\n",
        "            running_loss += loss.item()  # Add the batch loss to the running loss\n",
        "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")  # Print the epoch loss\n",
        "\n",
        "def evaluate_model(model):\n",
        "    \"\"\"\n",
        "    Evaluates the given model on the test data and prints the classification report.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The model to be evaluated.\n",
        "    \"\"\"\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    y_true, y_pred = [], []  # Initialize lists to store true labels and predicted labels\n",
        "    with torch.no_grad():  # Disable gradient computation to save memory\n",
        "        for x, y in test_loader:  # Iterate over the test data batch by batch\n",
        "            x = x.to(device)  # Move the data to the device\n",
        "            preds = torch.argmax(model(x), dim=1)  # Get the predicted labels\n",
        "            y_true.extend(y.numpy())  # Add the true labels to the list\n",
        "            y_pred.extend(preds.cpu().numpy())  # Add the predicted labels to the list\n",
        "\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names, digits=4))  # Print the classification report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3Tbtw2brCI5"
      },
      "source": [
        "# ===================== Q2 ====================="
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-n6oupDgrCI6"
      },
      "source": [
        "### Finetuned VGG19"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggxn_JPXkZL0",
        "outputId": "06701c19-a8c3-47d3-b591-07f9f65388ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 548M/548M [00:08<00:00, 70.2MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/10], Loss: 2.1129\n",
            "Epoch [2/10], Loss: 0.3782\n",
            "Epoch [3/10], Loss: 0.0777\n",
            "Epoch [4/10], Loss: 0.0243\n",
            "Epoch [5/10], Loss: 0.0069\n",
            "Epoch [6/10], Loss: 0.0069\n",
            "Epoch [7/10], Loss: 0.0047\n",
            "Epoch [8/10], Loss: 0.0020\n",
            "Epoch [9/10], Loss: 0.0008\n",
            "Epoch [10/10], Loss: 0.0008\n",
            "                precision    recall  f1-score   support\n",
            "\n",
            "     accordion     1.0000    1.0000    1.0000        15\n",
            "          bass     1.0000    1.0000    1.0000        14\n",
            "        camera     1.0000    1.0000    1.0000        10\n",
            "     crocodile     0.8000    0.8000    0.8000        10\n",
            "crocodile_head     0.8182    0.8182    0.8182        11\n",
            "           cup     1.0000    0.9412    0.9697        17\n",
            "   dollar_bill     1.0000    1.0000    1.0000        12\n",
            "           emu     1.0000    1.0000    1.0000        13\n",
            "    gramophone     1.0000    0.9091    0.9524        11\n",
            "      hedgehog     1.0000    0.9286    0.9630        14\n",
            "      nautilus     0.9375    1.0000    0.9677        15\n",
            "         pizza     1.0000    1.0000    1.0000        13\n",
            "       pyramid     1.0000    1.0000    1.0000        17\n",
            "     sea_horse     0.8947    1.0000    0.9444        17\n",
            " windsor_chair     1.0000    1.0000    1.0000        16\n",
            "\n",
            "      accuracy                         0.9659       205\n",
            "     macro avg     0.9634    0.9598    0.9610       205\n",
            "  weighted avg     0.9672    0.9659    0.9659       205\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Load a pre-trained VGG19 model\n",
        "vgg19_ft = models.vgg19(pretrained=True)\n",
        "\n",
        "# Set all the parameters in the features to require gradients\n",
        "# This allows us to update the model's weights during training\n",
        "for param in vgg19_ft.features.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Replace the last two layers of the pre-trained model with new linear layers\n",
        "# The first linear layer has 4096 inputs and 1024 outputs\n",
        "# The second linear layer has 1024 inputs and num_classes outputs\n",
        "vgg19_ft.classifier[5] = nn.Linear(4096, 1024)\n",
        "vgg19_ft.classifier[6] = nn.Linear(1024, num_classes)\n",
        "\n",
        "# Train the model for 10 epochs with a learning rate of 1e-5\n",
        "train_model(vgg19_ft, epochs=10, lr=1e-5)\n",
        "\n",
        "# Evaluate the trained model's performance on the test set\n",
        "evaluate_model(vgg19_ft)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "pytorch_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
